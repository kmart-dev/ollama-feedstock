{% set name = "ollama" %}
{% set version = "0.4.6" %}

package:
  name: {{ name|lower }}
  version: {{ version }}

source:
  url: https://github.com/ollama/ollama/releases/download/v{{ version }}/ollama-linux-amd64.tgz # [linux]
#  sha256: c9d5966db56dd7d248085d30ee9d97ca15b0de3dfa65d0d120100b4ed0176ebe 
  sha256: 1866b7fc11ee301dc3e488dc7d7631bd79e0e2a188919d7cdd1a02515087a672

build:
  number: 0
#  binary_relocation: false  # [osx]
#  skip: true
  script: ""
#    - mkdir -p {PREFIX}/bin {PREFIX}/lib/ollama
#    - cp -r ollama-linux-amd64.tgz $SRC_DIR/
#    - tar -xvf ollama-linux-amd64.tgz -C $SRC_DIR/
#    - cp -r $SRC_DIR/bin/* ${PREFIX}/bin/
#    - cp -r $SRC_DIR/lib/ollama/* ${PREFIX}/lib/ollama/

#requirements:
#  host:
#    - xz
#  run:
#    - xz

test:
  commands:
    - ollama --version
    - ollama --help

about:
  home: https://ollama.ai
  summary: Get up and running with Llama 2 and other large language models locally
  license: MIT
  license_family: MIT
  license_file:
    - LICENSE.txt
  dev_url: https://github.com/ollama/ollama
  description: |
    Ollama allows you to run open-source large language models locally.
    Get up and running with Llama 2, llama 2 uncensored, mistral, and other models.

extra:
  recipe-maintainers:
    - sodre
    - kmart-dev

